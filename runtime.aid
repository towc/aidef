runtime {
  path=src/runtime;

  Executes the compiled plan by running all leaf nodes in parallel.
  
  For each leaf, the runtime:
    1. Reads the .leaf.json file
    2. Creates the outputPath directory if needed
    3. Runs any whitelisted commands in that directory
    4. Generates files listed in files array, writing them to outputPath
    5. Prepends a generated file header to each file
  
  Generated file headers:
    The runtime deterministically prepends a comment header pointing to the source .aid file.
    Comment style depends on file extension (// for JS/TS, # for Python/YAML, etc.).
    JSON files get no header since JSON doesn't support comments.
    The source.aid path comes from the leaf's sourceAid field.
  
  Rate limiting and throttling:
    LLM providers have rate limits (tokens per minute, requests per minute).
    The runtime MUST handle 429 (rate limit) errors gracefully:
    1. Parse the retry delay from the error response (look for "retry.*(\d+)s" pattern)
    2. Wait the suggested delay plus a 5-second buffer
    3. Retry up to maxRetries times before failing the leaf
    4. Log every rate limit event with wait time and attempt number
    
    The runtime accepts a RateLimitConfig to enable proactive throttling:
    ```typescript
    export interface RateLimitConfig {
      tokensPerMinute?: number;   // e.g. 1000000 for Gemini 2.5 Flash
      requestsPerMinute?: number; // e.g. 1000 for Gemini 2.5 Flash
    }
    ```
    If provided, the runtime tracks token usage estimates and request counts,
    and delays requests proactively to stay under limits.
    
    After execution completes, the runtime writes a rate-limit summary to the log:
    - Total requests made
    - Total 429 errors encountered
    - Total time spent waiting on rate limits
    - Peak tokens/minute observed (estimated from prompt sizes)
    This helps users tune --max-parallel and understand their quota usage.
  
  Creates index.ts which exports a run function that takes outputDir, apiKey, and optional config.
  The config includes maxParallel, maxRetries, and rateLimits (RateLimitConfig).
  Finds all leaf.json files, executes them in parallel with progress reporting.
  
  CRITICAL CODE GENERATION RULES (applies to ALL modules in runtime):
  - NEVER use ambient declarations like "export const X: Type;" or "export function f(): void;"
  - Ambient declarations are for .d.ts files only, NOT implementation files
  - When you import something, use it directly - do NOT re-export or redeclare it
  - Only declare/export things you are IMPLEMENTING in this file
  - If the prompt shows interfaces for understanding, implement them OR import them - never stub them
  - If a function uses "await", it MUST be declared with "async"
  - If a function returns Promise, it MUST be declared with "async"

  leafExecutor {
    Executes a single leaf node using Google Gemini via @google/genai package (GoogleGenAI class, NOT @google/generative-ai or GoogleGenerativeAI).
    
    Exports executeLeaf function and GenLeaf interface.
    
    File system operations: use fs from 'node:fs/promises' for mkdir, readFile, etc.
    Do NOT use Bun.mkdir or other non-existent Bun APIs.
    Use Bun.file() for reading files, and Bun.write() for writing files.
    Use fs.mkdir(path, { recursive: true }) for creating directories.
    
    Uses generateContent API with write_file tool to generate each file.
    Maintains conversation history for multi-turn.
    Continues until all files in the leaf are written.
    Logs each file write to the logger.
    
    MUST handle 429 rate limit errors with retry and backoff:
    - Catch errors where status === 429
    - Parse retry delay from error message (pattern: "retry.*(\d+(?:\.\d+)?)s")
    - Wait the parsed delay + 5 second buffer
    - Retry up to maxRetries times
    - Log each retry attempt with wait time
    
    CRITICAL API PATTERN - use EXACTLY this code:
    ```typescript
    import { GoogleGenAI, Type } from '@google/genai';
    const genai = new GoogleGenAI({ apiKey });
    const response = await genai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: conversationHistory,  // array of messages
      config: { 
        temperature: 0,
        tools: [{ functionDeclarations: [{ 
          name: 'write_file', 
          description: 'Write content to a file',
          parameters: { 
            type: Type.OBJECT, 
            properties: { 
              path: { type: Type.STRING }, 
              content: { type: Type.STRING } 
            }, 
            required: ['path', 'content'] 
          } 
        }] }]
      }
    });
    // Access function calls: response.functionCalls (array of {name, args} or undefined)
    // For multi-turn: add function calls/responses to history and call generateContent again
    ```
    IMPORTANT: tools MUST be inside config object, NOT at the top level.
    NEVER use getGenerativeModel(), model.startChat(), or @google/generative-ai patterns.
    
    The code generation LLM must NEVER DIVERGE from the leaf prompt instructions.
    Follow them to the letter. When the prompt specifies exact package names, class names,
    function signatures, or API patterns, use them EXACTLY as written.
  }

  commandWhitelist {
    Leaf nodes can request shell commands, but only whitelisted ones run.
    Users can configure additional allowed commands in .aidrc or project config.
    The analyse mode suggests new commands to whitelist based on what leaves tried to run.
    
    Exports DEFAULT_WHITELIST array and isAllowed function.
    Default allows: bun init, bun install, bun add.
  }

  logger {
    Stores all AI calls and command executions in runtime.log.jsonl.
    Each line is a JSON object with timestamp, type, leaf path, and details.
    Used by analyse mode to review what happened.
    
    Exports (use EXACTLY these interfaces):
    ```typescript
    export interface LogEntry {
      timestamp: string;
      type: 'ai_call' | 'command_execution' | 'file_write' | 'error' | 'rate_limit' | 'summary';
      leafPath: string;
      details: any;
    }
    
    export interface Logger {
      log(entry: Omit<LogEntry, 'timestamp'>): void;
    }
    
    export function createLogger(logFilePath: string): Logger;
    ```
    
    Implementation:
      - createLogger MUST ensure parent directory of logFilePath exists before writing
      - Use fs.mkdirSync(dirname(logFilePath), { recursive: true }) from 'node:fs' and 'node:path'
      - Use fs.appendFileSync(filePath, logLine) from 'node:fs' for appending
      - Do NOT use Bun.write with append option - it does not exist
      - log() is synchronous - adds timestamp and writes JSON line to the file
  }
}
